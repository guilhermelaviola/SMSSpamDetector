{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbdni5uWAbFJvPxi2x7ne8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/SMSSpamDetector/blob/main/BagOfWords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most Machine Learning algorithms rely on numerical data to be fed into them as input. Emails and SMS messages are usually heavy texts, and in our case we have a large dataser (with 5572 rows of data).\n",
        "\n",
        "The **bag-of-words** model can help us to extract features from the text and use it in Maxhine Learning algorthms.\n",
        "\n",
        "In the **bag-of-words** approach we use the tokenized words for each ibservation and find out the frequency of each token.\n",
        "\n",
        "By doing that, we can convert the collection (dataset) into a frequency distribution matrix, with each document being a row and each word(token) being the column, and the corresponding (row,column) values being the frequency of occurrence of each word or token in that document.\n",
        "\n",
        "We will be using sklearn count vectorizer method which does the following:\n",
        "- It tokenizes the string(separates the string into individual words) and gives an integer ID to each token.\n",
        "- It counts the occurrence of each of those tokens."
      ],
      "metadata": {
        "id": "lL6yIeuX1TH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries\n",
        "import string\n",
        "import pprint\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "r7X_dF7Q53yQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting all strings into their lowercase form:\n",
        "documents = ['Hello, how are you!',\n",
        "             'Win money, win from home.',\n",
        "             'Call me now.',\n",
        "             'Hello, Call hello you tomorrow?']\n",
        "\n",
        "lower_case_documents = []\n",
        "lower_case_documents = [d.lower() for d in documents]\n",
        "print(lower_case_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR7dk4BZ59As",
        "outputId": "4a1d2d0d-bcf2-4e12-89e1-92372e1def88"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing all punctuations:\n",
        "sans_punctuation_documents = []\n",
        "\n",
        "for i in lower_case_documents:\n",
        "  sans_punctuation_documents.append(i.translate(str.maketrans('', '', string.punctuation)))\n",
        "\n",
        "sans_punctuation_documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MegGuzf15-dI",
        "outputId": "152710e1-a832-425c-f7a0-32d48f892a73"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello how are you',\n",
              " 'win money win from home',\n",
              " 'call me now',\n",
              " 'hello call hello you tomorrow']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "preprocessed_documents = [[w for w in d.split()] for d in sans_punctuation_documents]\n",
        "preprocessed_documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XizjX3F86Es_",
        "outputId": "e8855a78-9830-406f-8fd0-649d96b09a72"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['hello', 'how', 'are', 'you'],\n",
              " ['win', 'money', 'win', 'from', 'home'],\n",
              " ['call', 'me', 'now'],\n",
              " ['hello', 'call', 'hello', 'you', 'tomorrow']]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting frequency\n",
        "frequency_list = []\n",
        "\n",
        "frequency_list = [Counter(d) for d in preprocessed_documents]\n",
        "pprint.pprint(frequency_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EXL1T7q6IMH",
        "outputId": "32ace717-3fde-4ce1-ace8-621ef58fc22c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Counter({'hello': 1, 'how': 1, 'are': 1, 'you': 1}),\n",
            " Counter({'win': 2, 'money': 1, 'from': 1, 'home': 1}),\n",
            " Counter({'call': 1, 'me': 1, 'now': 1}),\n",
            " Counter({'hello': 2, 'call': 1, 'you': 1, 'tomorrow': 1})]\n"
          ]
        }
      ]
    }
  ]
}